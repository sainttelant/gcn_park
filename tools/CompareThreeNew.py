import torch
import onnx
import numpy as np
from PIL import Image
from torchvision import transforms
import pprint
from psdet.utils.config import get_config
from psdet.utils.common import get_logger
from psdet.models.builder import build_model
from pathlib import Path
import onnxruntime
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit

class TensorRTEngineValidator:
    def __init__(self, engine_path):
        """åˆå§‹åŒ–TensorRTå¼•æ“éªŒè¯å™¨"""
        self.engine = self._load_engine(engine_path)
        self.context = self.engine.create_execution_context()
        self.inputs, self.outputs, self.bindings, self.stream = self._allocate_buffers()

    def _load_engine(self, engine_path):
        """åŠ è½½å¹¶ååºåˆ—åŒ–TensorRTå¼•æ“"""
        TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
        try:
            with open(engine_path, 'rb') as f:
                engine_data = f.read()
            runtime = trt.Runtime(TRT_LOGGER)
            return runtime.deserialize_cuda_engine(engine_data)
        except Exception as e:
            raise RuntimeError(f"å¼•æ“åŠ è½½å¤±è´¥: {str(e)}") from e

    def _allocate_buffers(self):
        """åˆ†é…è¾“å…¥/è¾“å‡ºç¼“å†²åŒºï¼ˆæ”¯æŒåŠ¨æ€å½¢çŠ¶ï¼‰"""
        inputs, outputs, bindings = [], [], []
        stream = cuda.Stream()

        for binding_idx in range(self.engine.num_bindings):
            binding_name = self.engine.get_tensor_name(binding_idx)
            dtype = self.engine.get_tensor_dtype(binding_name)
            shape = self.context.get_tensor_shape(binding_name)

            # å¤„ç†æœªæŒ‡å®šå½¢çŠ¶çš„æƒ…å†µ
            if -1 in shape:
                shape = [max(dim, 1) for dim in shape]  # ç¡®ä¿æ²¡æœ‰é›¶å€¼
                self.context.set_input_shape(binding_name, shape)

            # è®¡ç®—ç¼“å†²åŒºå¤§å°
            dtype_size = np.dtype(trt.nptype(dtype)).itemsize
            num_elements = int(np.prod(shape))
            size_bytes = int(num_elements * dtype_size)
            print(f"Allocating memory of size: {size_bytes}, type: {type(size_bytes)}")
            print(f"Binding name: {binding_name}, shape: {shape}, dtype: {dtype}")
            if size_bytes <= 0:
                raise ValueError(f"Invalid buffer size for binding {binding_name}")

            # åˆ†é…è®¾å¤‡å†…å­˜
            device_mem = cuda.mem_alloc(size_bytes)
            bindings.append(int(device_mem))

            # åˆ†é…ä¸»æœºå†…å­˜ï¼ˆé”é¡µå†…å­˜åŠ é€Ÿä¼ è¾“ï¼‰
            host_mem = cuda.pagelocked_empty(num_elements, trt.nptype(dtype))

            if self.engine.binding_is_input(binding_idx):
                inputs.append({'host': host_mem, 'device': device_mem, 'name': binding_name})
            else:
                outputs.append({'host': host_mem, 'device': device_mem, 'name': binding_name})

        return inputs, outputs, bindings, stream

    def infer(self, input_data):
        """æ‰§è¡ŒTensorRTæ¨ç†"""
        # éªŒè¯è¾“å…¥å½¢çŠ¶
        input_shape = input_data.shape
        binding_shape = self.context.get_tensor_shape(self.inputs[0]['name'])
        if list(input_shape) != binding_shape:
            self.context.set_input_shape(self.inputs[0]['name'], input_shape)
            self.inputs, self.outputs, self.bindings, self.stream = self._allocate_buffers()

        # æ•°æ®é¢„å¤„ç†ï¼ˆç¡®ä¿è¿ç»­å†…å­˜ï¼‰
        input_data = np.ascontiguousarray(input_data)
        np.copyto(self.inputs[0]['host'], input_data.ravel())

        # ä¸»æœº->è®¾å¤‡ä¼ è¾“
        cuda.memcpy_htod_async(self.inputs[0]['device'], self.inputs[0]['host'], self.stream)

        # æ‰§è¡Œæ¨ç†
        self.context.execute_async_v2(bindings=self.bindings, stream_handle=self.stream.handle)

        # è®¾å¤‡->ä¸»æœºä¼ è¾“
        for output in self.outputs:
            cuda.memcpy_dtoh_async(output['host'], output['device'], self.stream)

        # åŒæ­¥æµ
        self.stream.synchronize()

        # é‡æ„è¾“å‡ºå½¢çŠ¶
        results = []
        for output in self.outputs:
            shape = self.context.get_tensor_shape(output['name'])
            results.append(output['host'].reshape(shape))

        return results

    @staticmethod
    def compare_outputs(trt_outputs, ref_outputs, tolerance=1e-2):
        """å¯¹æ¯”TensorRTè¾“å‡ºä¸å‚è€ƒè¾“å‡º"""
        report = {'passed': True, 'details': []}

        for i, (trt_out, ref_out) in enumerate(zip(trt_outputs, ref_outputs)):
            # ç¡®ä¿å‚è€ƒè¾“å‡ºä¸ºnumpyæ•°ç»„
            if isinstance(ref_out, torch.Tensor):
                ref_out = ref_out.detach().cpu().numpy()

            # å½¢çŠ¶éªŒè¯
            if trt_out.shape != ref_out.shape:
                report['passed'] = False
                report['details'].append({
                    'index': i,
                    'status': 'FAILED âŒ',
                    'message': f"å½¢çŠ¶ä¸åŒ¹é…: TRT={trt_out.shape} vs REF={ref_out.shape}",
                    'max_diff': None,
                    'mean_diff': None
                })
                continue

            # æ•°å€¼å·®å¼‚è®¡ç®—
            diff = np.abs(trt_out - ref_out)
            max_diff = np.max(diff)
            mean_diff = np.mean(diff)

            # åˆ¤æ–­æ˜¯å¦é€šè¿‡
            status = 'PASSED âœ…' if mean_diff < tolerance else 'FAILED âŒ'
            if status == 'FAILED âŒ':
                report['passed'] = False

            report['details'].append({
                'index': i,
                'status': status,
                'message': f"æœ€å¤§å·®å¼‚: {max_diff:.6f}, å¹³å‡å·®å¼‚: {mean_diff:.6f}",
                'max_diff': max_diff,
                'mean_diff': mean_diff
            })

        return report

    def cleanup(self):
        """é‡Šæ”¾æ‰€æœ‰CUDAèµ„æº"""
        for buf in self.inputs + self.outputs:
            if 'device' in buf and buf['device']:
                buf['device'].free()
                
def export_gnn_edge_model(model, cfg, device_id=0):
    device = torch.device(f'cuda:{device_id}')
    
    # GNNè¾¹é¢„æµ‹æ¨¡å‹åŒ…è£…å™¨
    class GNNEdgeWrapper(torch.nn.Module):
  
        def __init__(self, graph_encoder, edge_predictor):
            super().__init__()
            self.graph_encoder = graph_encoder
            self.edge_predictor = edge_predictor
            
        def forward(self, descriptors, points):
            data_dict = {
                'descriptors': descriptors,
                'points': points[:, :, :2]  # ä»…å–xyåæ ‡[6](@ref)
            }
            
            # å¤„ç†GNNåˆ†æ”¯
            if self.graph_encoder is not None:
                data_dict = self.graph_encoder(data_dict)
                graph_output = data_dict['descriptors']
            else:
                graph_output = descriptors
            
            # è¾¹é¢„æµ‹
            data_dict = self.edge_predictor(data_dict)
            return graph_output, data_dict['edges_pred']
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        # åˆ›å»ºåŒ…è£…å™¨å®ä¾‹
    submodel = GNNEdgeWrapper(
        graph_encoder=model.model.graph_encoder if cfg.use_gnn else None,
        edge_predictor=model.model.edge_predictor  # ä¿®æ­£å±æ€§è·¯å¾„[1](@ref)
    ).to(device)
    submodel.eval()
        
        # å…¶ä½™ä»£ç ä¿æŒä¸å˜...
    gnn_out_dim = cfg.model.graph_encoder.gnn.proj_dim
    B, N = 1, cfg.model.max_points
    dummy_desc = torch.randn(B, gnn_out_dim, N).to(device)  # [1, 64, 10]
    dummy_points = torch.rand(B, N, 2).to(device)     
        
        # å¯¼å‡ºONNX
    torch.onnx.export(
            submodel,
            (dummy_desc, dummy_points),
            str(Path(cfg.save_onnx_dir) / "gnn_edge_model.onnx"),
            export_params=True,
            opset_version=14,
            input_names=['descriptors', 'points'],
            output_names=['graph_output', 'edge_pred'],
            dynamic_axes={
                'descriptors': {0: 'batch_size', 2: 'num_points'},
                'points': {0: 'batch_size', 1: 'num_points'},
                'graph_output': {0: 'batch_size', 2: 'num_points'},
                'edge_pred': {0: 'batch_size', 2: 'num_edges'}
        }
    )
    print(f"GNNè¾¹é¢„æµ‹æ¨¡å‹å·²å¯¼å‡ºåˆ° {cfg.save_onnx_dir/'gnn_edge_model.onnx'}")


def export_model_to_onnx(model, cfg, device_id=0):
    # è®¾ç½®ä½¿ç”¨çš„è®¾å¤‡
    device = torch.device(f'cuda:{device_id}')

    # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡å¹¶è®¾ä¸ºè¯„ä¼°æ¨¡å¼
    model.to(device)
    model.eval()

    # æ¸…ç†ç¼“å­˜
    torch.cuda.empty_cache()

    # 1. å‡†å¤‡çœŸå®å›¾åƒè¾“å…¥
    #image_dir = Path(cfg.data_root) / 'testing' / 'indoor-parking lot'
    
    image_dir = Path("images")
    image = Image.open(image_dir/"001.jpg").convert("RGB")

    # åˆ›å»ºä¸æ¨¡å‹é¢„æœŸä¸€è‡´çš„é¢„å¤„ç†ç®¡é“
    preprocess = transforms.Compose([
        transforms.Resize((cfg.image_size[0], cfg.image_size[1])),
        transforms.ToTensor(),
    ])

    # åº”ç”¨é¢„å¤„ç†
    image_tensor = preprocess(image).unsqueeze(0).to(device)

    # 2. åˆ›å»ºè¾“å…¥å­—å…¸
    input_dict = {'image': image_tensor}

    # å®šä¹‰ä¿å­˜ ONNX æ¨¡å‹çš„è·¯å¾„
    onnx_model_path = cfg.save_onnx_dir
    simplified_path = cfg.simplified_path

    # åˆ›å»ºä¸€ä¸ªå¥å£®çš„æ¨¡å‹åŒ…è£…å™¨
    class ModelWrapper(torch.nn.Module):
        def __init__(self, model):
            super().__init__()
            self.model = model

        def forward(self, image):
            input_dict = {'image': image}
            data_dict = self.model(input_dict)
            points_pred = data_dict['points_pred']     
           

            if 'descriptor_map' in data_dict:
                descriptor_map = data_dict['descriptor_map']
            else:
                descriptor_map = torch.zeros(
                    image.size(0),
                    cfg.model.get('descriptor_dim', 256),
                    image.size(2) // cfg.model.get('depth_factor', 8),
                    image.size(3) // cfg.model.get('depth_factor', 8)
                ).to(image.device)

            return points_pred, descriptor_map

    # åŒ…è£…æ¨¡å‹å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    wrapped_model = ModelWrapper(model).to(device)
    wrapped_model.eval()

    # åœ¨å¯¼å‡ºå‰æµ‹è¯•ä¸€æ¬¡å‰å‘ä¼ æ’­
    with torch.no_grad():
        points_pred, descriptor_map = wrapped_model(image_tensor)
        print(f"points_pred shape: {points_pred.shape}")
        print(f"descriptor_map shape: {descriptor_map.shape}")
       

    # å¯¼å‡ºæ¨¡å‹åˆ° ONNX æ ¼å¼
    torch.onnx.export(
        wrapped_model,
        image_tensor,
        onnx_model_path/'model.onnx',
        export_params=True,
        opset_version=14,
        do_constant_folding=True,
        input_names=['image'],
        output_names=['points_pred', 'descriptor_map'],
        dynamic_axes={
            'image': {0: 'batch_size', 2: 'height', 3: 'width'},
            'points_pred': {0: 'batch_size'},
            'descriptor_map': {0: 'batch_size'}
        }
    )
    print(f"æ¨¡å‹å·²è½¬æ¢ä¸º ONNX å¹¶ä¿å­˜åˆ° {onnx_model_path}")

    # éªŒè¯ ONNX æ¨¡å‹
    onnx_model = onnx.load(onnx_model_path/"model.onnx")

    for input in onnx_model.graph.input:
        a = input
        #print(a)
        for dim in input.type.tensor_type.shape.dim:
            aa = dim
            print(aa.dim_value)
        print(f"Input: {input.name}, Shape: {[dim.dim_value for dim in input.type.tensor_type.shape.dim]}")
        print("ONNX æ¨¡å‹éªŒè¯é€šè¿‡")

    # ä¼˜åŒ– ONNX æ¨¡å‹
    try:
        from onnxsim import simplify
        simplified_model, check = simplify(onnx_model)
        onnx.save(simplified_model, simplified_path/"model_simplified.onnx")
        print(f"ONNXæ¨¡å‹ç®€åŒ–å®Œæˆï¼Œä¿å­˜åˆ°: {simplified_path}")
        
        model = onnx.load(simplified_path/"model.onnx")
        for input in model.graph.input:
            if input.name == "image":  # æ›¿æ¢ä¸ºä½ çš„è¾“å…¥åç§°
                # æ¸…é™¤åŠ¨æ€ç»´åº¦æ ‡è®°
                input.type.tensor_type.shape.dim[0].dim_value = 1  # batch
                input.type.tensor_type.shape.dim[2].dim_value = 512  # height
                input.type.tensor_type.shape.dim[3].dim_value = 512  # width
        onnx.save(model, simplified_path/"fixed_model.onnx")
        
    except ImportError:
        print("è­¦å‘Š: onnx-simplifier æœªå®‰è£…ï¼Œè·³è¿‡æ¨¡å‹ç®€åŒ–æ­¥éª¤")
    except Exception as e:
        print(f"æ¨¡å‹ç®€åŒ–å¤±è´¥: {e}")
        
    

    # éªŒè¯ ONNX æ¨¡å‹    
    validation_result = validate_onnx_model(wrapped_model, image_tensor, onnx_model_path)
    if validation_result:
        print("\n[ğŸ‰] ONNXæ¨¡å‹éªŒè¯æˆåŠŸï¼ä¸åŸæ¨¡å‹å®Œå…¨ä¸€è‡´")
    else:
        print("\n[âš ï¸] è­¦å‘Šï¼šONNXæ¨¡å‹è¾“å‡ºä¸åŸå§‹æ¨¡å‹å­˜åœ¨å·®å¼‚")

    # ç›´æ¥åŠ è½½å·²æœ‰çš„TensorRTå¼•æ“è¿›è¡ŒéªŒè¯
    validate_trt = False
    if validate_trt: 
        trt_validator = TensorRTEngineValidator(engine_path=onnx_model_path/"new.engine")
        trt_outputs = trt_validator.infer(image_tensor.cpu().numpy())

        # è·å–PyTorchåŸå§‹è¾“å‡º
        with torch.no_grad():
            torch_outputs = wrapped_model(image_tensor)

        # è½¬æ¢ä¸ºå¯æ¯”è¾ƒæ ¼å¼
        torch_outputs = [t.cpu().numpy() if isinstance(t, torch.Tensor) else t for t in torch_outputs]

        # æ¯”è¾ƒç»“æœ
        report = trt_validator.compare_outputs(trt_outputs, torch_outputs)

        # æ‰“å°æ¯”è¾ƒç»“æœ
        print("\n" + "="*50)
        print(f"TensorRTéªŒè¯ç»“æœ: {'é€šè¿‡' if report['passed'] else 'å¤±è´¥'}")
        print("="*50)
        for detail in report['details']:
            print(f"è¾“å‡º {detail['index']}: {detail['status']}")
            print(f"  â†’ {detail['message']}")

        # é‡Šæ”¾èµ„æº
        trt_validator.cleanup()

def validate_onnx_model(wrapped_model, image_tensor, onnx_model_path):
    # 1. åŠ è½½ONNXæ¨¡å‹å¹¶éªŒè¯åŸºç¡€ç»“æ„
    onnx_model = onnx.load(onnx_model_path/"model_simplified.onnx")
    try:
        onnx.checker.check_model(onnx_model)
        print("[âœ…] ONNXæ¨¡å‹åŸºç¡€ç»“æ„éªŒè¯é€šè¿‡")
    except onnx.checker.ValidationError as e:
        print(f"[âŒ] ONNXæ¨¡å‹ç»“æ„å¼‚å¸¸: {e}")
        return False

    # 2. å‡†å¤‡ONNX Runtimeæ¨ç†ä¼šè¯
    ort_session = onnxruntime.InferenceSession(
        str(onnx_model_path/"model_simplified.onnx"),
        providers=['CUDAExecutionProvider'] if torch.cuda.is_available() else ['CPUExecutionProvider']
    )
    
     # 3. è·å–è¾“å…¥è¾“å‡ºåç§°
    input_name = ort_session.get_inputs()[0].name
    output_names = [out.name for out in ort_session.get_outputs()]
    
    ort_inputs = {input_name: image_tensor.cpu().numpy()}
    ort_outs = ort_session.run(output_names, ort_inputs)
    print(f"ONNX points_pred shape: {ort_outs[0].shape}")
    print(f"ONNX descriptor_map shape: {ort_outs[1].shape}")
   

    # 4. è¿è¡ŒPyTorchåŸå§‹æ¨¡å‹æ¨ç†
    with torch.no_grad():
        torch_points, torch_desc = wrapped_model(image_tensor)

        print("PyTorch pointsæ€»å’Œ:", torch_points.sum().item())
        print("PyTorch pointsç»Ÿè®¡: æ€»å’Œ={:.4f}, æœ€å¤§={:.4f}, æœ€å°={:.4f}".format(
            torch_points.sum().item(),
            torch_points.max().item(),
            torch_points.min().item()
        ))
        
        print("pytorch descæ€»å’Œ:", torch_desc.sum().item())
        print("pytorch descç»Ÿè®¡: æ€»å’Œ={:.4f}, æœ€å¤§={:.4f}, æœ€å°={:.4f}".format(
            torch_desc.sum().item(),
            torch_desc.max().item(),
            torch_desc.min().item()
        ))
        if not isinstance(torch_points, torch.Tensor):
            torch_points = torch.tensor(torch_points).to(image_tensor.device)

    # 5. è¿è¡ŒONNXæ¨¡å‹æ¨ç†
    ort_inputs = {input_name: image_tensor.cpu().numpy()}
    ort_outs = ort_session.run(output_names, ort_inputs)
    onnx_points = torch.from_numpy(ort_outs[0]).to(image_tensor.device)
    onnx_desc = torch.from_numpy(ort_outs[1]).to(image_tensor.device)

    # 6. æ•°å€¼ä¸€è‡´æ€§éªŒè¯
    passed = True
    # éªŒè¯å…³é”®ç‚¹è¾“å‡º
    points_diff = torch.abs(torch_points - onnx_points)
    max_diff = points_diff.max().item()
    mean_diff = points_diff.mean().item()
    print(f"å…³é”®ç‚¹è¾“å‡ºå·®å¼‚: æœ€å¤§={max_diff:.6f}, å¹³å‡={mean_diff:.6f}")
    if mean_diff > 1e-2:
        print("[âŒ] å…³é”®ç‚¹è¾“å‡ºå·®å¼‚è¿‡å¤§!")
        passed = False

    # éªŒè¯æè¿°ç¬¦è¾“å‡º
    desc_diff = torch.abs(torch_desc - onnx_desc)
    max_desc_diff = desc_diff.max().item()
    mean_desc_diff = desc_diff.mean().item()
    print(f"æè¿°ç¬¦è¾“å‡ºå·®å¼‚: æœ€å¤§={max_desc_diff:.6f}, å¹³å‡={mean_desc_diff:.6f}")
    if mean_desc_diff > 1e-2:
        print("[âŒ] æè¿°ç¬¦è¾“å‡ºå·®å¼‚è¿‡å¤§!")
        passed = False

    return passed

if __name__ == '__main__':
    cfg = get_config()
    logger = get_logger(cfg.log_dir, cfg.tag)
    logger.info(pprint.pformat(cfg))

    model = build_model(cfg.model)
    logger.info(model)
    model.load_params_from_file(filename=cfg.ckpt, logger=logger, to_cpu=False)

    device = torch.device('cuda:0')
    model.to(device)

    #export_model_to_onnx(model, cfg, device_id=0)
    export_gnn_edge_model(model, cfg, device_id=0)