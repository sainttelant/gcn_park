import torch
import onnx
import numpy as np
from PIL import Image
from torchvision import transforms
import pprint
from psdet.utils.config import get_config
from psdet.utils.common import get_logger
from psdet.models.builder import build_model
from pathlib import Path
import onnxruntime 
from torchsummary import summary

def export_model_to_onnx(model, cfg, device_id=0):
    # è®¾ç½®ä½¿ç”¨çš„è®¾å¤‡
    device = torch.device(f'cuda:{device_id}')
    
    # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡å¹¶è®¾ä¸ºè¯„ä¼°æ¨¡å¼
    model.to(device)
    model.eval()

    # æ¸…ç†ç¼“å­˜
    torch.cuda.empty_cache()

    # 1. å‡†å¤‡çœŸå®å›¾åƒè¾“å…¥
    #image_path = cfg.test_image  # å‡è®¾é…ç½®ä¸­æœ‰æµ‹è¯•å›¾åƒè·¯å¾„
    image_dir = Path(cfg.data_root) / 'testing' / 'indoor-parking lot'
    image = Image.open(image_dir/"001.jpg").convert("RGB")
    
    # åˆ›å»ºä¸æ¨¡å‹é¢„æœŸä¸€è‡´çš„é¢„å¤„ç†ç®¡é“
    preprocess = transforms.Compose([
        transforms.Resize((cfg.image_size[0], cfg.image_size[1])),  # ä½¿ç”¨é…ç½®ä¸­çš„å›¾åƒå°ºå¯¸
        transforms.ToTensor(),          # è½¬æ¢ä¸ºTensor
    ])
    
    # åº”ç”¨é¢„å¤„ç†
    image_tensor = preprocess(image).unsqueeze(0).to(device)  # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
    
    # 2. åˆ›å»ºè¾“å…¥å­—å…¸
    input_dict = {'image': image_tensor}
    
    # å®šä¹‰ä¿å­˜ ONNX æ¨¡å‹çš„è·¯å¾„
    onnx_model_path = cfg.save_onnx_dir
    simplified_path = cfg.simplified_path
    # åˆ›å»ºä¸€ä¸ªå¥å£®çš„æ¨¡å‹åŒ…è£…å™¨
    class ModelWrapper(torch.nn.Module):
        def __init__(self, model):
            super().__init__()
            self.model = model
            
        def forward(self, image):
            # åˆ›å»ºè¾“å…¥å­—å…¸
            input_dict = {'image': image}
            
            # è°ƒç”¨æ¨¡å‹
            data_dict = self.model(input_dict)
            
            # æå–æ¨¡å‹è¾“å‡º
            points_pred = data_dict['points_pred']
            
            # è·å–æè¿°ç¬¦å›¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if 'descriptor_map' in data_dict:
                descriptor_map = data_dict['descriptor_map']
            else:
                # å¯¹äº DirectionalPointDetectorï¼Œæè¿°ç¬¦å›¾å¯èƒ½ä¸å¯ç”¨
                descriptor_map = torch.zeros(
                    image.size(0), 
                    cfg.model['descriptor_dim'] if hasattr(cfg, 'descriptor_dim') else 256,
                    image.size(2) // (cfg.model['depth_factor'] if hasattr(cfg, 'depth_factor') else 8),
                    image.size(3) // (cfg.model['depth_factor'] if hasattr(cfg, 'depth_factor') else 8)
                ).to(image.device)
            
            return points_pred, descriptor_map
    
    # åŒ…è£…æ¨¡å‹å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    wrapped_model = ModelWrapper(model).to(device)
    wrapped_model.eval()

    # åœ¨å¯¼å‡ºå‰æµ‹è¯•ä¸€æ¬¡å‰å‘ä¼ æ’­
    with torch.no_grad():
        points_pred, descriptor_map = wrapped_model(image_tensor)
        print("è¾“å‡ºç»“æ„:")
        # å®‰å…¨åœ°æ‰“å° points_pred çš„ä¿¡æ¯
        if isinstance(points_pred, torch.Tensor):
            print(f"points_pred: ç±»å‹=å¼ é‡, å½¢çŠ¶={points_pred.shape}")
        elif isinstance(points_pred, (list, tuple)):
            print(f"points_pred: ç±»å‹={type(points_pred)}, é•¿åº¦={len(points_pred)}")
            # æ‰“å°åˆ—è¡¨ä¸­ç¬¬ä¸€ä¸ªå…ƒç´ çš„ç±»å‹ï¼ˆå¦‚æœæœ‰ï¼‰
            if len(points_pred) > 0:
                first_item = points_pred[0]
                print(f"  ç¬¬ä¸€ä¸ªå…ƒç´ çš„ç±»å‹: {type(first_item)}")
                if isinstance(first_item[0], tuple):
                    print(f"  ç¬¬ä¸€ä¸ªå…ƒç»„æœ‰ {len(first_item)} ä¸ªå…ƒç´ ")
                    if len(first_item) > 0:
                        print(f"    ç¬¬ä¸€ä¸ªå…ƒç´ çš„ç½®ä¿¡åº¦: {first_item[0]}")
                        if isinstance(first_item[1], np.ndarray):
                            print(f"    ä½ç½®æ•°ç»„çš„å½¢çŠ¶: {first_item[1].shape}")
        else:
            print(f"points_pred: ç±»å‹={type(points_pred)}")
        
        # å®‰å…¨åœ°æ‰“å° descriptor_map çš„ä¿¡æ¯
        if descriptor_map is not None and isinstance(descriptor_map, torch.Tensor):
            print(f"descriptor_map: ç±»å‹=å¼ é‡, å½¢çŠ¶={descriptor_map.shape}")
        else:
            print(f"descriptor_map: {type(descriptor_map)}")
                    
    # å¯¼å‡ºæ¨¡å‹åˆ° ONNX æ ¼å¼
    torch.onnx.export(
        wrapped_model, 
        image_tensor,
        onnx_model_path/'model.onnx',
        export_params=True,
        opset_version=14,  # ä½¿ç”¨æ›´æ–°çš„ opset ç‰ˆæœ¬
        do_constant_folding=True,
        input_names=['image'],
        output_names=['points_pred', 'descriptor_map'],
        dynamic_axes={
            'image': {0: 'batch_size', 2: 'height', 3: 'width'},
            'points_pred': {0: 'batch_size'},
            'descriptor_map': {0: 'batch_size'}
        }
    )
    print(f"æ¨¡å‹å·²è½¬æ¢ä¸º ONNX å¹¶ä¿å­˜åˆ° {onnx_model_path}")

    # éªŒè¯ ONNX æ¨¡å‹
    onnx_model = onnx.load(onnx_model_path/"model.onnx")
  
    for input in onnx_model.graph.input:
        print(f"Input: {input.name}, Shape: {[dim.dim_value for dim in input.type.tensor_type.shape.dim]}")
        print("ONNX æ¨¡å‹éªŒè¯é€šè¿‡")

    # å¯é€‰ï¼šä¼˜åŒ– ONNX æ¨¡å‹
     # å°è¯•ç®€åŒ–æ¨¡å‹
    try:
        from onnxsim import simplify
        simplified_model, check = simplify(onnx_model)
        onnx.save(simplified_model, simplified_path/"model_simplified.onnx")
        print(f"ONNXæ¨¡å‹ç®€åŒ–å®Œæˆï¼Œä¿å­˜åˆ°: {simplified_path}")
    except ImportError:
        print("è­¦å‘Š: onnx-simplifier æœªå®‰è£…ï¼Œè·³è¿‡æ¨¡å‹ç®€åŒ–æ­¥éª¤")
    except Exception as e:
        print(f"æ¨¡å‹ç®€åŒ–å¤±è´¥: {e}")
    
   
    
    # éªŒè¯ ONNX æ¨¡å‹    
    validation_result = validate_onnx_model(wrapped_model, image_tensor, onnx_model_path)
    if validation_result:
        print("\n[ğŸ‰] ONNXæ¨¡å‹éªŒè¯æˆåŠŸï¼ä¸åŸæ¨¡å‹å®Œå…¨ä¸€è‡´")
    else:
        print("\n[âš ï¸] è­¦å‘Šï¼šONNXæ¨¡å‹è¾“å‡ºä¸åŸå§‹æ¨¡å‹å­˜åœ¨å·®å¼‚")
   
def validate_onnx_model(wrapped_model, image_tensor, onnx_model_path):
    # 1. åŠ è½½ONNXæ¨¡å‹å¹¶éªŒè¯åŸºç¡€ç»“æ„
    onnx_model = onnx.load(onnx_model_path/"model_simplified.onnx")
    try:
        onnx.checker.check_model(onnx_model)  # æ£€æŸ¥æ¨¡å‹æ˜¯å¦ç¬¦åˆONNXæ ‡å‡†
        print("[âœ…] ONNXæ¨¡å‹åŸºç¡€ç»“æ„éªŒè¯é€šè¿‡")
    except onnx.checker.ValidationError as e:
        print(f"[âŒ] ONNXæ¨¡å‹ç»“æ„å¼‚å¸¸: {e}")
        return False

    # 2. å‡†å¤‡ONNX Runtimeæ¨ç†ä¼šè¯
    ort_session = onnxruntime.InferenceSession(
        str(onnx_model_path/"model_simplified.onnx"),
        providers=['CUDAExecutionProvider'] if torch.cuda.is_available() else ['CPUExecutionProvider']
    )
    
    # 3. è·å–è¾“å…¥è¾“å‡ºåç§°
    input_name = ort_session.get_inputs()[0].name
    output_names = [out.name for out in ort_session.get_outputs()]
    
    # 4. è¿è¡ŒPyTorchåŸå§‹æ¨¡å‹æ¨ç†
    with torch.no_grad():
        torch_points, torch_desc = wrapped_model(image_tensor)
        
        print("PyTorch pointsæ€»å’Œ:", torch_points.sum().item())
        print("PyTorch pointsç»Ÿè®¡: æ€»å’Œ={:.4f}, æœ€å¤§={:.4f}, æœ€å°={:.4f}".format(
            torch_points.sum().item(),
            torch_points.max().item(),
            torch_points.min().item()
        ))
        # ç¡®ä¿è¾“å‡ºä¸ºå¼ é‡ä»¥ä¾¿æ¯”è¾ƒ
        if not isinstance(torch_points, torch.Tensor):
            torch_points = torch.tensor(torch_points).to(image_tensor.device)
    
    # 5. è¿è¡ŒONNXæ¨¡å‹æ¨ç†
    ort_inputs = {input_name: image_tensor.cpu().numpy()}
    ort_outs = ort_session.run(output_names, ort_inputs)
    onnx_points = torch.from_numpy(ort_outs[0]).to(image_tensor.device)
    onnx_desc = torch.from_numpy(ort_outs[1]).to(image_tensor.device)

    # 6. æ•°å€¼ä¸€è‡´æ€§éªŒè¯ï¼ˆå…³é”®æ­¥éª¤ï¼‰
    passed = True
    # éªŒè¯å…³é”®ç‚¹è¾“å‡º
    points_diff = torch.abs(torch_points - onnx_points)
    max_diff = points_diff.max().item()
    mean_diff = points_diff.mean().item()
    print(f"å…³é”®ç‚¹è¾“å‡ºå·®å¼‚: æœ€å¤§={max_diff:.6f}, å¹³å‡={mean_diff:.6f}")
    if mean_diff > 1e-2:  # è®¾ç½®åˆç†é˜ˆå€¼
        print("[âŒ] å…³é”®ç‚¹è¾“å‡ºå·®å¼‚è¿‡å¤§!")
        passed = False
    
    # éªŒè¯æè¿°ç¬¦è¾“å‡º
    desc_diff = torch.abs(torch_desc - onnx_desc)
    max_desc_diff = desc_diff.max().item()
    mean_desc_diff = desc_diff.mean().item()
    print(f"æè¿°ç¬¦è¾“å‡ºå·®å¼‚: æœ€å¤§={max_desc_diff:.6f}, å¹³å‡={mean_desc_diff:.6f}")
    if mean_desc_diff > 1e-2:
        print("[âŒ] æè¿°ç¬¦è¾“å‡ºå·®å¼‚è¿‡å¤§!")
        passed = False
    
    # 7. åŠ¨æ€å½¢çŠ¶éªŒè¯ï¼ˆå¯é€‰ï¼‰
    if cfg.validate_dynamic_shape:
        print("\n[ğŸ§ª] åŠ¨æ€å½¢çŠ¶éªŒè¯...")
        try:
            # åˆ›å»ºä¸åŒæ‰¹æ¬¡çš„è¾“å…¥
            batch2_input = torch.cat([image_tensor, image_tensor], dim=0)
            ort_inputs_batch2 = {input_name: batch2_input.cpu().numpy()}
            ort_outs_batch2 = ort_session.run(output_names, ort_inputs_batch2)
            print(f"[âœ…] åŠ¨æ€æ‰¹æ¬¡éªŒè¯é€šè¿‡ (batch=2)")
        except Exception as e:
            print(f"[âŒ] åŠ¨æ€å½¢çŠ¶éªŒè¯å¤±è´¥: {str(e)}")
            passed = False
    
    return passed    
    
if __name__ == '__main__':
    cfg = get_config()
    logger = get_logger(cfg.log_dir, cfg.tag)
    logger.info(pprint.pformat(cfg))
    
    model = build_model(cfg.model)
    logger.info(model)
    model.load_params_from_file(filename=cfg.ckpt, logger=logger, to_cpu=False)

    device = torch.device('cuda:0')
    model.to(device)

    export_model_to_onnx(model, cfg, device_id=0)
    